{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------------------+\n",
      "|year|month|         difference|\n",
      "+----+-----+-------------------+\n",
      "|2014|   12| -1.838561940815072|\n",
      "|2014|   11| 2.9202591036414574|\n",
      "|2014|   10| 2.6506731357636166|\n",
      "|2014|    9|-0.9561130563336455|\n",
      "|2014|    8|0.34340513857551613|\n",
      "|2014|    7| 0.7707767430662216|\n",
      "|2014|    6| 0.5867637800883099|\n",
      "|2014|    5| 1.1918083405986657|\n",
      "|2014|    4| 1.8507542360173952|\n",
      "|2014|    3| 3.8866128652955414|\n",
      "|2014|    2| 3.4851983771565043|\n",
      "|2014|    1| 3.1917645844887845|\n",
      "|2013|   12|  4.672728381765573|\n",
      "|2013|   11| 0.3435924369747902|\n",
      "|1990|    7| 2.6110993237113824|\n",
      "|1990|    2|   62.8101983771565|\n",
      "|1970|    7| 16.111099323711382|\n",
      "|1970|    2| 22.810198377156503|\n",
      "|1961|    5| 1.0272486195873274|\n",
      "|1961|    4|  1.581865347128507|\n",
      "+----+-----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "sc.stop()\n",
    "sc = SparkContext(appName=\"exercise\")\n",
    "file=sc.textFile(\"temperature-readings-tiny.csv\")\n",
    "linesp = file.map(lambda line:line.split(\";\"))\n",
    "\n",
    "stations_Ostergotland=sc.textFile(\"stations.csv\")\n",
    "liness = stations_Ostergotland.map(lambda line:line.split(\";\"))\n",
    "\n",
    "######## the list of all the stations\n",
    "stations = liness.map(lambda x:x[0]).distinct().collect()  \n",
    "\n",
    "################ step 1 ###############\n",
    "daily_temp = linesp.map(lambda x:((x[1][0:7],x[0]),float(x[3])))\n",
    "daily_temp = daily_temp.filter(lambda x: str(x[0][1]) in stations)\n",
    "month_temp_sum = daily_temp.reduceByKey(lambda v1,v2:v1+v2)    \n",
    "# print(*month_temp_sum.take(10),sep=\"\\n\")\n",
    "\n",
    "num_month = daily_temp.map(lambda x:(x[0], 1))    \n",
    "num_month = num_month.reduceByKey(lambda v1,v2:v1+v2)       \n",
    "# print(*num_month.take(10), sep=\"\\n\")\n",
    "month_temp_ave = month_temp_sum.union(num_month).reduceByKey(lambda v1,v2:v1/v2)\n",
    "# print(*month_temp_ave.take(10),sep=\"\\n\")\n",
    "\n",
    "################ step 2 ###############\n",
    "month_temp_ave2 = month_temp_ave.map(lambda x: (x[0][0], x[1]))\n",
    "month_temp_ave2 = month_temp_ave2.reduceByKey(lambda v1,v2: v1+v2)\n",
    "num_station = month_temp_ave.map(lambda x: (x[0][0], 1))\n",
    "num_station = num_station.reduceByKey(lambda v1,v2:v1+v2)\n",
    "month_temp_ave2 = month_temp_ave2.union(num_station).reduceByKey(lambda v1,v2:v1/v2)\n",
    "month_temp_ave2 = month_temp_ave2.map(lambda x: [int(x[0][0:4]), int(x[0][5:7]), x[1]])\n",
    "# print(*month_temp_ave2.take(10),sep=\"\\n\")\n",
    "\n",
    "############### final ##################\n",
    "# list[year, month, value]\n",
    "ave1 = month_temp_ave2.filter(lambda x: x[0]>=1950 and x[0]<=2014)\\\n",
    "\t\t\t\t\t  .sortBy(lambda x: (x[0],[1]), ascending = False)\n",
    "final = ave1.collect()\n",
    "# dict(month: value)\n",
    "ave2 = month_temp_ave2.filter(lambda x: x[0]>=1950 and x[0]<=1980)\\\n",
    "\t\t\t\t\t  .sortBy(lambda x: (x[0],[1]), ascending = False)\n",
    "sum2 = ave2.map(lambda x: (x[1], x[2])).reduceByKey(lambda v1, v2: v1+v2)\n",
    "num2 = ave2.map(lambda x: (x[1], 1)).reduceByKey(lambda v1, v2: v1+v2)\n",
    "ave2 = dict(sum2.union(num2).reduceByKey(lambda v1,v2:v1/v2).collect())\n",
    "\n",
    "for i in range(len(final)):\n",
    "    month = final[i][1]\n",
    "    final[i][2] -= ave2[month]\n",
    "    \n",
    "lll = sc.parallelize(final)\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.createDataFrame(lll)\n",
    "df = df.sort(df._1.desc(), df._2.desc()).withColumnRenamed(\"_1\",\"year\")\\\n",
    "\t\t\t\t\t\t\t .withColumnRenamed(\"_2\",\"month\")\\\n",
    "\t\t\t\t\t\t\t .withColumnRenamed(\"_3\",\"difference\")\n",
    "df.show()\n",
    "# fff = df.rdd\n",
    "# fff.saveAsTextFile(\"./results1/1q6\") \n",
    "\n",
    "#with open(\"1q6\", \"w\") as file:\n",
    "#    for element in final:\n",
    "#        file.write(\"%s, %s, %s \\n\" % (element[0], element[1], element[2]))\n",
    "        \n",
    "#a = ave1.map(lambda x: (str(x[0])+\"-\"+str(x[1]), x[2]))\n",
    "#kk = a.keys().collect()\n",
    "#fig, ax = plt.subplots(figsize=(8,4))\n",
    "#ax.plot(kk, [x[2] for x in final])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: -2.8101983771565044,\n",
       " 4: 2.925912430649273,\n",
       " 6: 13.856569553245023,\n",
       " 8: 13.525949700134163,\n",
       " 10: 4.45577847713961,\n",
       " 12: -3.9630509624107337,\n",
       " 7: 18.888900676288618,\n",
       " 1: -7.298216197392009,\n",
       " 3: -1.9898386717471543,\n",
       " 5: 9.564643272304563,\n",
       " 9: 9.541113056333645,\n",
       " 11: -0.39525910364145683}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ave2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
